{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Untitled55.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyMv9gT52ruFmmEG6nLqVorQ",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/jaeyukkim/TF-study/blob/main/DenseNet(MNIST).ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "C071w2icGqiT",
        "outputId": "6ccb6378-0f20-478e-e707-e5f2344217fd"
      },
      "source": [
        "import tensorflow as tf\n",
        "import tensorflow_datasets as tfds \n",
        "\n",
        "from termcolor import colored\n",
        "from tensorflow.keras.layers import BatchNormalization, Concatenate\n",
        "from tensorflow.keras.layers import Conv2D, MaxPool2D, Flatten, Dense\n",
        "from tensorflow.keras.metrics import Mean, SparseCategoricalAccuracy\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.losses import SparseCategoricalCrossentropy\n",
        "\n",
        "class DenseUnit(Model):\n",
        "  def __init__(self, filter_out, kernel_size):\n",
        "    super(DenseUnit, self).__init__()\n",
        "    self.bn = BatchNormalization()\n",
        "    self.conv2d = Conv2D(filter_out, kernel_size, padding='same')\n",
        "    self.concat = Concatenate()\n",
        "\n",
        "  def call(self, x, training=False, mask=None):\n",
        "    h = self.bn(x, training==training)\n",
        "    h = tf.nn.relu(h)\n",
        "    h = self.conv2d(h)\n",
        "    h = self.concat([x, h])\n",
        "\n",
        "    return h\n",
        "\n",
        "\n",
        "class DenseLayer(Model):\n",
        "  def __init__(self, num_unit, growth_rate, kernel_size):\n",
        "    super(DenseLayer, self).__init__()\n",
        "    self.sequential = list()\n",
        "    for i in range(num_unit):\n",
        "      self.sequential.append(DenseUnit(growth_rate, kernel_size))\n",
        "  \n",
        "  def call(self, x, training=False, mask=None):\n",
        "    for layer in self.sequential:\n",
        "      x = layer(x, training==training)\n",
        "    \n",
        "    return x\n",
        "\n",
        "\n",
        "class TransitionLayer(Model):\n",
        "  def __init__(self, filters, kernel_size):\n",
        "    super(TransitionLayer, self).__init__()\n",
        "    self.conv = Conv2D(filters, kernel_size, padding='same')\n",
        "    self.pool = MaxPool2D()\n",
        "  \n",
        "  def call(self, x, training=False, Mask=None):\n",
        "    x = self.conv(x)\n",
        "    x = self.pool(x)\n",
        "\n",
        "    return x\n",
        "\n",
        "\n",
        "\n",
        "class DenseNet(Model):\n",
        "  def __init__(self):\n",
        "    super(DenseNet, self).__init__()\n",
        "    self.conv1 = Conv2D(8, (3,3), padding='same', activation='relu')    #28x28x8\n",
        "    \n",
        "    self.dense1 = DenseLayer(2, 4, (3,3))       #28x28x16\n",
        "    self.trans1 = TransitionLayer(16,(3,3))             #14x14x16\n",
        "\n",
        "    self.dense2 = DenseLayer(2, 8, (3,3))       #14x14x32\n",
        "    self.trans2 = TransitionLayer(16,(3,3))             #7x7x32\n",
        "\n",
        "    self.dense3 = DenseLayer(2, 16, (3,3))       #7x7x64\n",
        "\n",
        "    self.flatten = Flatten()\n",
        "    self.fully_conected1 = Dense(128, activation='relu')\n",
        "    self.fully_conected2 = Dense(10, activation='softmax')\n",
        "\n",
        "    \n",
        "  def call(self, x, training=False, Mask=None):\n",
        "    x = self.conv1(x)\n",
        "\n",
        "    x = self.dense1(x, training==training)\n",
        "    x = self.trans1(x)\n",
        "\n",
        "    x = self.dense2(x, training==training)\n",
        "    x = self.trans2(x)\n",
        "\n",
        "    x = self.dense3(x, training==training)\n",
        "  \n",
        "    x = self.flatten(x)\n",
        "    x = self.fully_conected1(x)\n",
        "    x = self.fully_conected2(x)\n",
        "\n",
        "    return x\n",
        "\n",
        "\n",
        "#===============================================================================================\n",
        "\n",
        "def load_dataset():\n",
        "  (train_validation_ds, test_ds) ,ds_info = tfds.load(name='mnist',\n",
        "                                                      split=['train', 'test'],\n",
        "                                                      as_supervised=True,\n",
        "                                                      with_info=True,\n",
        "                                                      shuffle_files = True,\n",
        "                                                      batch_size = None)\n",
        "  \n",
        "  n_train_validation_ds = ds_info.splits['train'].num_examples\n",
        "  train_ratio = 0.8\n",
        "  n_train = int(n_train_validation_ds * train_ratio)\n",
        "  n_validation = n_train_validation_ds - n_train\n",
        "\n",
        "  train_ds = train_validation_ds.take(n_train)\n",
        "  remain_ds = train_validation_ds.skip(n_train)\n",
        "  validation_ds = remain_ds.take(n_validation)\n",
        " \n",
        "  return train_ds, validation_ds, test_ds, ds_info\n",
        "\n",
        "\n",
        "def normalization(TRAIN_BATCH_SIZE, TEST_BATCH_SIZE):\n",
        "  global train_ds, validation_ds, test_ds\n",
        "  \n",
        "  def norm(images, labels):\n",
        "    images = tf.cast(images, tf.float32) / 255.\n",
        "    return [images, labels]\n",
        "  \n",
        "  train_ds = train_ds.map(norm).shuffle(1000).batch(TRAIN_BATCH_SIZE)\n",
        "  validation_ds = validation_ds.map(norm).batch(TEST_BATCH_SIZE)\n",
        "  test_ds = test_ds.map(norm).batch(TEST_BATCH_SIZE)\n",
        "\n",
        "#-------------------------------------------------------------------------------\n",
        "\n",
        "def load_matrics():\n",
        "  global train_loss, validation_loss, test_loss\n",
        "  global train_acc, validation_acc, test_acc\n",
        "  \n",
        "  train_loss = Mean()\n",
        "  validation_loss = Mean()\n",
        "  test_loss = Mean()\n",
        "\n",
        "  train_acc = SparseCategoricalAccuracy()\n",
        "  validation_acc = SparseCategoricalAccuracy()\n",
        "  test_acc = SparseCategoricalAccuracy()\n",
        "\n",
        "\n",
        "@tf.function\n",
        "def training():\n",
        "  global train_ds, train_loss, train_acc\n",
        "  global loss_object, optimizer, model\n",
        "\n",
        "  for images, labels in train_ds:\n",
        "    with tf.GradientTape() as tape:\n",
        "      predictions = model(images, training=True)\n",
        "      loss = loss_object(labels, predictions)\n",
        "    \n",
        "    gradients = tape.gradient(loss, model.trainable_variables)\n",
        "    optimizer.apply_gradients(zip(gradients, model.trainable_variables))\n",
        "\n",
        "    train_loss(loss)\n",
        "    train_acc(labels, predictions)\n",
        "\n",
        "\n",
        "@tf.function\n",
        "def validation():\n",
        "  global validation_ds, validation_acc, validation_loss\n",
        "  global loss_object, model\n",
        "\n",
        "  for images, labels in validation_ds:      \n",
        "    predictions = model(images, training=False)\n",
        "    loss = loss_object(labels, predictions)\n",
        "      \n",
        "    validation_loss(loss)\n",
        "    validation_acc(labels, predictions)\n",
        "\n",
        "\n",
        "@tf.function\n",
        "def tester():\n",
        "  global test_ds, test_acc, test_loss\n",
        "  global loss_object, model\n",
        "\n",
        "  for images, labels in test_ds:      \n",
        "    predictions = model(images, training=False)\n",
        "    loss = loss_object(labels, predictions)\n",
        "      \n",
        "    test_loss(loss)\n",
        "    test_acc(labels, predictions)\n",
        "\n",
        "\n",
        "def train_result_and_reset_state():\n",
        "  global epoch\n",
        "  global train_loss, train_acc\n",
        "  global validation_loss, validation_acc\n",
        "\n",
        "  print(colored('Epochs', 'red', 'on_white'), epoch + 1)\n",
        "  temp = 'Train Loss : {:.4f}\\t Train Accuracy : {:.2f}%\\n' +\\\n",
        "         'Validation Loss : {:.4f}\\t Validation Accuracy : {:.2f}%\\n'\n",
        "  \n",
        "  print(temp.format(train_loss.result(),\n",
        "                    train_acc.result()*100,\n",
        "                    validation_loss.result(),\n",
        "                    validation_acc.result()*100))\n",
        "\n",
        "  train_loss.reset_states()\n",
        "  train_acc.reset_states()\n",
        "  validation_loss.reset_states()\n",
        "  validation_acc.reset_states()\n",
        "\n",
        "\n",
        "EPOCHS = 20\n",
        "#LR = 0.001\n",
        "TRAIN_BATCH_SIZE = 100\n",
        "TEST_BATCH_SIZE = 100\n",
        "\n",
        "optimizer = Adam()\n",
        "loss_object = SparseCategoricalCrossentropy()\n",
        "\n",
        "train_ds, validation_ds, test_ds, ds_info = load_dataset()\n",
        "normalization(TRAIN_BATCH_SIZE, TEST_BATCH_SIZE)\n",
        "\n",
        "model = DenseNet()\n",
        "model.build(input_shape=(None, 28, 28, 1))\n",
        "load_matrics()\n",
        "\n",
        "for epoch in range(EPOCHS):  \n",
        "  training()\n",
        "  validation()\n",
        "  train_result_and_reset_state()\n",
        "\n",
        "tester()\n",
        "print(colored('Epochs', 'cyan', 'on_white') , epoch + 1)\n",
        "print('============Test Result============')\n",
        "temp = 'TEST LOSS : {:.4f}\\t TEST ACC : {:.2f}%\\n'\n",
        "print(temp.format(test_loss.result(),\n",
        "                  test_acc.result()*100))\n"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\u001b[47m\u001b[31mEpochs\u001b[0m 1\n",
            "Train Loss : 0.1453\t Train Accuracy : 95.60%\n",
            "Validation Loss : 0.0473\t Validation Accuracy : 98.43%\n",
            "\n",
            "\u001b[47m\u001b[31mEpochs\u001b[0m 2\n",
            "Train Loss : 0.0479\t Train Accuracy : 98.49%\n",
            "Validation Loss : 0.0571\t Validation Accuracy : 98.19%\n",
            "\n",
            "\u001b[47m\u001b[31mEpochs\u001b[0m 3\n",
            "Train Loss : 0.0313\t Train Accuracy : 99.01%\n",
            "Validation Loss : 0.0487\t Validation Accuracy : 98.72%\n",
            "\n",
            "\u001b[47m\u001b[31mEpochs\u001b[0m 4\n",
            "Train Loss : 0.0261\t Train Accuracy : 99.15%\n",
            "Validation Loss : 0.0407\t Validation Accuracy : 98.77%\n",
            "\n",
            "\u001b[47m\u001b[31mEpochs\u001b[0m 5\n",
            "Train Loss : 0.0201\t Train Accuracy : 99.33%\n",
            "Validation Loss : 0.0422\t Validation Accuracy : 98.73%\n",
            "\n",
            "\u001b[47m\u001b[31mEpochs\u001b[0m 6\n",
            "Train Loss : 0.0185\t Train Accuracy : 99.45%\n",
            "Validation Loss : 0.0719\t Validation Accuracy : 98.12%\n",
            "\n",
            "\u001b[47m\u001b[31mEpochs\u001b[0m 7\n",
            "Train Loss : 0.0205\t Train Accuracy : 99.36%\n",
            "Validation Loss : 0.0618\t Validation Accuracy : 98.33%\n",
            "\n",
            "\u001b[47m\u001b[31mEpochs\u001b[0m 8\n",
            "Train Loss : 0.0172\t Train Accuracy : 99.47%\n",
            "Validation Loss : 0.0608\t Validation Accuracy : 98.53%\n",
            "\n",
            "\u001b[47m\u001b[31mEpochs\u001b[0m 9\n",
            "Train Loss : 0.0145\t Train Accuracy : 99.56%\n",
            "Validation Loss : 0.0619\t Validation Accuracy : 98.61%\n",
            "\n",
            "\u001b[47m\u001b[31mEpochs\u001b[0m 10\n",
            "Train Loss : 0.0138\t Train Accuracy : 99.57%\n",
            "Validation Loss : 0.0637\t Validation Accuracy : 98.60%\n",
            "\n",
            "\u001b[47m\u001b[31mEpochs\u001b[0m 11\n",
            "Train Loss : 0.0140\t Train Accuracy : 99.54%\n",
            "Validation Loss : 0.0546\t Validation Accuracy : 98.88%\n",
            "\n",
            "\u001b[47m\u001b[31mEpochs\u001b[0m 12\n",
            "Train Loss : 0.0175\t Train Accuracy : 99.46%\n",
            "Validation Loss : 0.0654\t Validation Accuracy : 98.74%\n",
            "\n",
            "\u001b[47m\u001b[31mEpochs\u001b[0m 13\n",
            "Train Loss : 0.0122\t Train Accuracy : 99.66%\n",
            "Validation Loss : 0.0586\t Validation Accuracy : 98.83%\n",
            "\n",
            "\u001b[47m\u001b[31mEpochs\u001b[0m 14\n",
            "Train Loss : 0.0129\t Train Accuracy : 99.65%\n",
            "Validation Loss : 0.0609\t Validation Accuracy : 98.95%\n",
            "\n",
            "\u001b[47m\u001b[31mEpochs\u001b[0m 15\n",
            "Train Loss : 0.0110\t Train Accuracy : 99.70%\n",
            "Validation Loss : 0.0770\t Validation Accuracy : 98.58%\n",
            "\n",
            "\u001b[47m\u001b[31mEpochs\u001b[0m 16\n",
            "Train Loss : 0.0134\t Train Accuracy : 99.66%\n",
            "Validation Loss : 0.0651\t Validation Accuracy : 98.82%\n",
            "\n",
            "\u001b[47m\u001b[31mEpochs\u001b[0m 17\n",
            "Train Loss : 0.0108\t Train Accuracy : 99.73%\n",
            "Validation Loss : 0.0670\t Validation Accuracy : 98.85%\n",
            "\n",
            "\u001b[47m\u001b[31mEpochs\u001b[0m 18\n",
            "Train Loss : 0.0132\t Train Accuracy : 99.66%\n",
            "Validation Loss : 0.0813\t Validation Accuracy : 98.59%\n",
            "\n",
            "\u001b[47m\u001b[31mEpochs\u001b[0m 19\n",
            "Train Loss : 0.0123\t Train Accuracy : 99.69%\n",
            "Validation Loss : 0.0637\t Validation Accuracy : 98.94%\n",
            "\n",
            "\u001b[47m\u001b[31mEpochs\u001b[0m 20\n",
            "Train Loss : 0.0122\t Train Accuracy : 99.67%\n",
            "Validation Loss : 0.0668\t Validation Accuracy : 98.88%\n",
            "\n",
            "\u001b[47m\u001b[36mEpochs\u001b[0m 20\n",
            "============Test Result============\n",
            "TEST LOSS : 0.0714\t TEST ACC : 98.93%\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}