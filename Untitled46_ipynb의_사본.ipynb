{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Untitled46.ipynb의 사본",
      "provenance": [],
      "authorship_tag": "ABX9TyNnBoLtj44C7Dl5d2vmdEM/",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/jaeyukkim/TF-study/blob/main/Untitled46_ipynb%EC%9D%98_%EC%82%AC%EB%B3%B8.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kLht4lg6wr2J",
        "outputId": "029e0305-7824-4385-94c9-14b0cabbde3b"
      },
      "source": [
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from termcolor import colored\n",
        "import tensorflow_datasets as tfds \n",
        "\n",
        "from tensorflow.keras.losses import SparseCategoricalCrossentropy\n",
        "from tensorflow.keras.optimizers import SGD, Adam\n",
        "from tensorflow.keras.layers import Dense, Conv2D, MaxPooling2D\n",
        "from tensorflow.keras.layers import Activation, Flatten, Dropout\n",
        "from tensorflow.keras.metrics import SparseCategoricalAccuracy, Mean\n",
        "from tensorflow.keras.models import Model, Sequential\n",
        "\n",
        "class Residualunit(tf.keras.Model):\n",
        "  def __init__(self, filter_in, filter_out, kernel_size):\n",
        "    super(Residualunit, self).__init__()\n",
        "    self.bn1 = BatchNormalization()\n",
        "    self.conv1 = Conv2D(filter_out, kernel_size, padding='same')\n",
        "\n",
        "    self.bn2 = BatchNormalization()\n",
        "    self.conv2 = Conv2D(filter_out, kernel_size, padding='same')\n",
        "\n",
        "    if filter_in == filter_out:\n",
        "      self.inentity = lambda x: x\n",
        "    else:\n",
        "      self.identity = Conv2D(filter_out, (1,1), padding='same')\n",
        "\n",
        "  def call(self, x, training=False, mask=None):\n",
        "    h = self.bn1(x, training=training)\n",
        "    h = tf.nn.relu(h)\n",
        "    h = self.conv1(h)\n",
        "\n",
        "    h = self.bn2(h, training=training)\n",
        "    h = tf.nn.relu(h)\n",
        "    h = self.conv2(h)\n",
        "\n",
        "    return self.identity(x) + h\n",
        "\n",
        "\n",
        "class ResnetLayer(Model):\n",
        "  def __init__(self, filter_in, filters, kernel_size):\n",
        "    super(ResnetLayer, self).__init__()\n",
        "    self.sequence = list()\n",
        "    for f_in, f_out in zip([filter_in] + list(filters), filters):\n",
        "      self.sequence.append(Residualunit(f_in, f_out, kernel_size))\n",
        "\n",
        "  def call(self, x, training=False, mask=None):\n",
        "    for unit in self.sequence:\n",
        "      x = unit(x, training=training)\n",
        "      return x\n",
        "\n",
        "\n",
        "class ResNet(Model):\n",
        "  def __init__(self):\n",
        "    super(ResNet, self).__init__()\n",
        "    self.conv1 = Conv2D(8, (3, 3), padding = 'same', activation = 'relu') #28X28X8\n",
        "\n",
        "    self.res1 = ResnetLayer(8, (16,16), (3,3)) #28X28X16\n",
        "    self.pool1 = MaxPool2D((2,2)) #14X14X16\n",
        "\n",
        "    self.res2 = ResnetLayer(16, (32,32), (3,3)) #14X14X32\n",
        "    self.pool2 = MaxPool2D((2,2)) #7X7X32\n",
        "\n",
        "    self.res3 = ResnetLayer(32, (64,64), (3,3)) #7X7X64\n",
        "    self.flatten = Flatten()\n",
        "\n",
        "    self.dense1 = Dense(128, activation = 'relu')\n",
        "    self.dense2 = Dense(10, activation = 'softmax')\n",
        "\n",
        "\n",
        "  def call(self, x, training=False, mask=None):\n",
        "    x = self.conv1(x)\n",
        "\n",
        "    x = self.res1(x, training=training)\n",
        "    x = self.pool1(x)\n",
        "\n",
        "    x = self.res2(x, training=training)\n",
        "    x = self.pool2(x)\n",
        "\n",
        "    x = self.res3(x, training=training)\n",
        "    x = self.flatten(x)\n",
        "\n",
        "    x = self.dense1(x)\n",
        "    x = self.dense2(x)\n",
        "    \n",
        "    return x\n",
        "\n",
        "\n",
        "#===============================================================================================\n",
        "\n",
        "def load_dataset():\n",
        "  (train_validation_ds, test_ds) ,ds_info = tfds.load(name='mnist',\n",
        "                                                      split=['train', 'test'],\n",
        "                                                      as_supervised=True,\n",
        "                                                      with_info=True,\n",
        "                                                      shuffle_files = True,\n",
        "                                                      batch_size = None)\n",
        "  \n",
        "  n_train_validation_ds = ds_info.splits['train'].num_examples\n",
        "  train_ratio = 0.8\n",
        "  n_train = int(n_train_validation_ds * train_ratio)\n",
        "  n_validation = n_train_validation_ds - n_train\n",
        "\n",
        "  train_ds = train_validation_ds.take(n_train)\n",
        "  remain_ds = train_validation_ds.skip(n_train)\n",
        "  validation_ds = remain_ds.take(n_validation)\n",
        " \n",
        "  return train_ds, validation_ds, test_ds, ds_info\n",
        "\n",
        "\n",
        "def normalization(TRAIN_BATCH_SIZE, TEST_BATCH_SIZE):\n",
        "  global train_ds, validation_ds, test_ds\n",
        "  \n",
        "  def norm(images, labels):\n",
        "    images = tf.cast(images, tf.float32) / 255.\n",
        "    return [images, labels]\n",
        "  \n",
        "  train_ds = train_ds.map(norm).shuffle(1000).batch(TRAIN_BATCH_SIZE)\n",
        "  validation_ds = validation_ds.map(norm).batch(TEST_BATCH_SIZE)\n",
        "  test_ds = test_ds.map(norm).batch(TEST_BATCH_SIZE)\n",
        "\n",
        "#-------------------------------------------------------------------------------\n",
        "\n",
        "def load_matrics():\n",
        "  global train_loss, validation_loss, test_loss\n",
        "  global train_acc, validation_acc, test_acc, loss_object\n",
        "  \n",
        "  train_loss = Mean()\n",
        "  validation_loss = Mean()\n",
        "  test_loss = Mean()\n",
        "\n",
        "  train_acc = SparseCategoricalAccuracy()\n",
        "  validation_acc = SparseCategoricalAccuracy()\n",
        "  test_acc = SparseCategoricalAccuracy()\n",
        "\n",
        "\n",
        "@tf.function\n",
        "def training():\n",
        "  global train_ds, train_loss, train_acc\n",
        "  global loss_object, optimizer, model\n",
        "\n",
        "  for images, labels in train_ds:\n",
        "    with tf.GradientTape() as tape:\n",
        "      predictions = model(images, training=True)\n",
        "      loss = loss_object(labels, predictions)\n",
        "    \n",
        "    gradients = tape.gradient(loss, model.trainable_variables)\n",
        "    optimizer.apply_gradients(zip(gradients, model.trainable_variables))\n",
        "\n",
        "    train_loss(loss)\n",
        "    train_acc(labels, predictions)\n",
        "\n",
        "\n",
        "@tf.function\n",
        "def validation():\n",
        "  global validation_ds, validation_acc, validation_loss\n",
        "  global loss_object, model\n",
        "\n",
        "  for images, labels in validation_ds:      \n",
        "    predictions = model(images, training=False)\n",
        "    loss = loss_object(labels, predictions)\n",
        "      \n",
        "    validation_loss(loss)\n",
        "    validation_acc(labels, predictions)\n",
        "\n",
        "\n",
        "@tf.function\n",
        "def tester():\n",
        "  global test_ds, test_acc, test_loss\n",
        "  global loss_object, model\n",
        "\n",
        "  for images, labels in test_ds:      \n",
        "    predictions = model(images, training=False)\n",
        "    loss = loss_object(labels, predictions)\n",
        "      \n",
        "    test_loss(loss)\n",
        "    test_acc(labels, predictions)\n",
        "\n",
        "\n",
        "def train_result_and_reset_state():\n",
        "  global epoch\n",
        "  global train_loss, train_acc\n",
        "  global validation_loss, validation_acc\n",
        "\n",
        "  print(colored('Epochs', 'red', 'on_white'), epoch + 1)\n",
        "  temp = 'Train Loss : {:.4f}\\t Train Accuracy : {:.2f}%\\n' +\\\n",
        "         'Validation Loss: {:.4f}\\t Validation Accuracy : {:.2f}%\\n'\n",
        "  \n",
        "  print(temp.format(train_loss.result(),\n",
        "                    train_acc.result()*100,\n",
        "                    validation_loss.result(),\n",
        "                    validation_acc.result()*100))\n",
        "\n",
        "  train_loss.reset_states()\n",
        "  train_acc.reset_states()\n",
        "  validation_loss.reset_states()\n",
        "  validation_acc.reset_states()\n",
        "\n",
        "\n",
        "EPOCHS = 20\n",
        "#LR = 0.001\n",
        "TRAIN_BATCH_SIZE = 100\n",
        "TEST_BATCH_SIZE = 100\n",
        "\n",
        "optimizer = Adam()\n",
        "loss_object = SparseCategoricalCrossentropy()\n",
        "\n",
        "train_ds, validation_ds, test_ds, ds_info = load_dataset()\n",
        "normalization(TRAIN_BATCH_SIZE, TEST_BATCH_SIZE)\n",
        "\n",
        "model = ResNet()\n",
        "model.build(input_shape=(None, 28, 28, 1))\n",
        "load_matrics()\n",
        "\n",
        "for epoch in range(EPOCHS):  \n",
        "  training()\n",
        "  validation()\n",
        "  train_result_and_reset_state()\n",
        "\n",
        "tester()\n",
        "print(colored('Epochs', 'cyan', 'on_white') , epoch + 1)\n",
        "print('============Test Result============')\n",
        "temp = 'TEST LOSS : {:.4f}\\t TEST ACC : {:.2f}%\\n'\n",
        "print(temp.format(test_loss.result(),\n",
        "                  test_acc.result()*100))\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\u001b[47m\u001b[31mEpochs\u001b[0m 1\n",
            "Train Loss : 0.1498\t Train Accuracy : 95.60%\n",
            "Validation Loss: 0.0693\t Validation Accuracy : 97.73%\n",
            "\n",
            "\u001b[47m\u001b[31mEpochs\u001b[0m 2\n",
            "Train Loss : 0.0461\t Train Accuracy : 98.61%\n",
            "Validation Loss: 0.0459\t Validation Accuracy : 98.65%\n",
            "\n",
            "\u001b[47m\u001b[31mEpochs\u001b[0m 3\n",
            "Train Loss : 0.0320\t Train Accuracy : 98.98%\n",
            "Validation Loss: 0.0376\t Validation Accuracy : 98.93%\n",
            "\n",
            "\u001b[47m\u001b[31mEpochs\u001b[0m 4\n",
            "Train Loss : 0.0330\t Train Accuracy : 98.95%\n",
            "Validation Loss: 0.0412\t Validation Accuracy : 98.78%\n",
            "\n",
            "\u001b[47m\u001b[31mEpochs\u001b[0m 5\n",
            "Train Loss : 0.0236\t Train Accuracy : 99.26%\n",
            "Validation Loss: 0.0587\t Validation Accuracy : 98.12%\n",
            "\n",
            "\u001b[47m\u001b[31mEpochs\u001b[0m 6\n",
            "Train Loss : 0.0197\t Train Accuracy : 99.40%\n",
            "Validation Loss: 0.0565\t Validation Accuracy : 98.52%\n",
            "\n",
            "\u001b[47m\u001b[31mEpochs\u001b[0m 7\n",
            "Train Loss : 0.0181\t Train Accuracy : 99.43%\n",
            "Validation Loss: 0.0575\t Validation Accuracy : 98.72%\n",
            "\n",
            "\u001b[47m\u001b[31mEpochs\u001b[0m 8\n",
            "Train Loss : 0.0210\t Train Accuracy : 99.40%\n",
            "Validation Loss: 0.0881\t Validation Accuracy : 98.10%\n",
            "\n",
            "\u001b[47m\u001b[31mEpochs\u001b[0m 9\n",
            "Train Loss : 0.0158\t Train Accuracy : 99.49%\n",
            "Validation Loss: 0.0919\t Validation Accuracy : 98.28%\n",
            "\n",
            "\u001b[47m\u001b[31mEpochs\u001b[0m 10\n",
            "Train Loss : 0.0180\t Train Accuracy : 99.47%\n",
            "Validation Loss: 0.0490\t Validation Accuracy : 99.09%\n",
            "\n",
            "\u001b[47m\u001b[31mEpochs\u001b[0m 11\n",
            "Train Loss : 0.0138\t Train Accuracy : 99.60%\n",
            "Validation Loss: 0.0598\t Validation Accuracy : 98.69%\n",
            "\n",
            "\u001b[47m\u001b[31mEpochs\u001b[0m 12\n",
            "Train Loss : 0.0088\t Train Accuracy : 99.73%\n",
            "Validation Loss: 0.0581\t Validation Accuracy : 98.92%\n",
            "\n",
            "\u001b[47m\u001b[31mEpochs\u001b[0m 13\n",
            "Train Loss : 0.0165\t Train Accuracy : 99.56%\n",
            "Validation Loss: 0.0724\t Validation Accuracy : 98.50%\n",
            "\n",
            "\u001b[47m\u001b[31mEpochs\u001b[0m 14\n",
            "Train Loss : 0.0188\t Train Accuracy : 99.48%\n",
            "Validation Loss: 0.0478\t Validation Accuracy : 99.07%\n",
            "\n",
            "\u001b[47m\u001b[31mEpochs\u001b[0m 15\n",
            "Train Loss : 0.0101\t Train Accuracy : 99.71%\n",
            "Validation Loss: 0.0563\t Validation Accuracy : 98.93%\n",
            "\n",
            "\u001b[47m\u001b[31mEpochs\u001b[0m 16\n",
            "Train Loss : 0.0101\t Train Accuracy : 99.72%\n",
            "Validation Loss: 0.0479\t Validation Accuracy : 99.08%\n",
            "\n",
            "\u001b[47m\u001b[31mEpochs\u001b[0m 17\n",
            "Train Loss : 0.0080\t Train Accuracy : 99.75%\n",
            "Validation Loss: 0.0750\t Validation Accuracy : 98.44%\n",
            "\n",
            "\u001b[47m\u001b[31mEpochs\u001b[0m 18\n",
            "Train Loss : 0.0125\t Train Accuracy : 99.66%\n",
            "Validation Loss: 0.0539\t Validation Accuracy : 98.86%\n",
            "\n",
            "\u001b[47m\u001b[31mEpochs\u001b[0m 19\n",
            "Train Loss : 0.0107\t Train Accuracy : 99.72%\n",
            "Validation Loss: 0.0622\t Validation Accuracy : 98.93%\n",
            "\n",
            "\u001b[47m\u001b[31mEpochs\u001b[0m 20\n",
            "Train Loss : 0.0105\t Train Accuracy : 99.70%\n",
            "Validation Loss: 0.0605\t Validation Accuracy : 99.01%\n",
            "\n",
            "\u001b[47m\u001b[36mEpochs\u001b[0m 20\n",
            "============Test Result============\n",
            "TEST LOSS : 0.0688\t TEST ACC : 99.07%\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gIu2kWEu6J3p"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}