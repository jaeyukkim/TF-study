import numpy as np
import tensorflow as tf
import tensorflow_datasets as tfds

import matplotlib.pyplot as plt
from termcolor import colored

from tensorflow.keras.losses import SparseCategoricalCrossentropy
from tensorflow.keras.models import Model,Sequential
from tensorflow.keras.layers import Dense, Flatten, Activation
from tensorflow.keras.metrics import SparseCategoricalAccuracy, Mean
from tensorflow.keras.optimizers import Adam, SGD

class MNIST_Classifier(Model): #model classification
  def __init__(self):
    super(MNIST_Classifier, self).__init__()

    self.flatten = Flatten()
    self.d1 = Dense(784, activation='softmax')
    self.d2 = Dense(50, activation='softmax')
    self.d3 = Dense(10, activation='softmax')

  def call(self, x):
    x = self.flatten(x)
    x = self.d1(x)
    x = self.d2(x)
    x = self.d3(x)
    return x

#----------------------------------------------------------------------------

def load_dataset(): #dataset load and separating train dataset, validation dataset dataset
  (train_validation_ds, test_ds), ds_info = tfds.load(name='mnist',
                                                      as_supervised=True,
                                                      shuffle_files=True,
                                                      split=['train', 'test'],
                                                      with_info = True)
  
  n_train_validation_ds = ds_info.splits['train'].num_examples
  
  train_ratio = 0.8
  n_train = int(n_train_validation_ds * train_ratio)
  n_validation_ds = n_train_validation_ds - n_train

  train_ds = train_validation_ds.take(n_train)
  remain_ds = train_validation_ds.skip(n_train)
  validation_ds = remain_ds.take(n_validation_ds)

  return train_ds, validation_ds, test_ds, n_train

  
def standardization(TRAIN_BATCH_SIZE, TEST_BATCH_SIZE): #Initialize all data from 0 to 1
  global train_ds, validation_ds, test_ds

  def std(images, labels):
    images = tf.cast(images, tf.float32) / 255.
    labels = tf.cast(labels, tf.int32)
    return [images, labels]

  train_ds = train_ds.map(std).shuffle(1000).batch(TRAIN_BATCH_SIZE)
  validation_ds = validation_ds.map(std).batch(TEST_BATCH_SIZE)
  test_ds = test_ds.map(std).batch(TEST_BATCH_SIZE)
    

#-----------------------------------------------------------------------------------------

def load_metrics(): 
  global train_loss, train_acc
  global validation_loss, validation_acc
  global test_loss, test_acc

  train_loss = Mean()
  validation_loss = Mean()
  test_loss = Mean()

  train_acc = SparseCategoricalAccuracy()
  validation_acc = SparseCategoricalAccuracy()
  test_acc = SparseCategoricalAccuracy()


@tf.function #Used to speed up
def training(): # training train_dataset
  global train_ds, model, loss_object, optimizer
  global train_loss, train_acc

  for images, labels in train_ds:
    with tf.GradientTape() as tape:
      predictions = model(images)
      loss = loss_object(labels, predictions)
    
    gradients = tape.gradient(loss, model.trainable_variables)
    optimizer.apply_gradients(zip(gradients, model.trainable_variables))

    train_loss(loss)
    train_acc(labels, predictions)


@tf.function #Used to speed up
def validation():
  global validation_ds, model, loss_object
  global validation_loss, validation_acc
  
  for images, labels in validation_ds:
      
    predictions = model(images)
    loss = loss_object(labels, predictions)

    validation_loss(loss)
    validation_acc(labels, predictions)


@tf.function #Used to speed up
def test_and_result():
  global test_ds, model, loss_object
  global test_loss, test_acc
  
  for images, labels in test_ds:      
    predictions = model(images)
    loss = loss_object(labels, predictions)

    test_loss(loss)
    test_acc(labels, predictions)
  

def train_result_and_reset_state():
  global epoch
  global train_loss, train_acc
  global validation_loss, validation_acc

  print(colored('Epochs', 'red', 'on_white'), epoch + 1)
  temp = 'Train Loss : {:.4f}\t Train Accuracy : {:.2f}%\n' +\
         'Validation Loss: {:.4f}\t Validation Accuracy : {:.2f}%\n'
  
  print(temp.format(train_loss.result(),
                    train_acc.result()*100,
                    validation_loss.result(),
                    validation_acc.result()*100))

  train_loss.reset_states()
  train_acc.reset_states()
  validation_loss.reset_states()
  validation_acc.reset_states()

#-------------------------------------------------------------------------------

EPOCHS = 30 #10000
#LR = 0.01
TRAIN_BATCH_SIZE = 100
TEST_BATCH_SIZE = 100

train_ds, validation_ds, test_ds, n_train = load_dataset()
standardization(TRAIN_BATCH_SIZE, TEST_BATCH_SIZE)

model = MNIST_Classifier()
loss_object = SparseCategoricalCrossentropy()
optimizer = Adam() #default 값 사용

load_metrics()

ITER_PER_EPOCHS = (n_train / TRAIN_BATCH_SIZE)

for epoch in range(EPOCHS):
  training()
  validation()
  #if epoch % ITER_PER_EPOCHS == 0:
  train_result_and_reset_state()

test_and_result()

print('========Test Result=======')
temp = 'Test Loss : {:.4f} \t Test_Accuracy : {:.2f}%\n'
print(temp.format(test_loss.result(),
                  test_acc.result()*100))
  
